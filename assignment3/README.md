# CT80A0000 Data-Intensive Systems - Assignment 3

## Assignment instructions
1. Create multiple databases (at least three) that have some dummy data within them (at least four tables with five data entities in each minimum).
    1. Any database type is allowed (PostgreSQL, SQLite, MongoDB, SQL Server, etc.)
    2. Have the databases have at least some of the same relations (tables), showing fragmentation and replication of data.
2. Create a frontend program that can access the databases (for example using Java, Python, JavaScript, etc.) 
3. Within the frontend, have the user be able to choose a location and print data from the database based on the chosen location
    1. The database used would be changed based on the location.


## Explanation

### Prerequisites & How to run 
*To run the database servers Docker, Docker compose and Python3 must be installed.*

1. To start all 3 databases run the command `docker compose up -d` in `./db`. The databases start on 127.0.0.1 (localhost) with the ports 5432 (db-1), 5433 (db-2) and 5434 (db-3)

2. When using docker compose to start the databases the `./db/docker-entrypoint-initdb.d/db{1,2,3}/init.sql` scripts are run in the respective databases automatically.

4. Create a `.env` file with the fields `CONN_STR_1, CONN_STR_2, CONN_STR_3` and fill them with the corresponding connection strigs (format `postgresql://[user]:[password]@[ip_addr]:[port]`)

5. Create a python virtual environment `python -m venv .venv` 

6. Activate the environment `source ./.venv/bin/activate` on Unix based systems.

7. Install python dependencies `pip install -r requirements.txt`

8. Start the frontend program `python main.py`

## General information
The "frontend" in this project is a simple python terminal application. The application is developed using Python 3.12.7, using other versions of python may work, but is not recommended.

The database connections in this project are done using psycopg3 and only 1 connection is active at once. During the switching of the database, the previous connection is closed before connecting to a new database. 

All tables in the database are fragmented by a hash of the ID into 2 partitions.

## Declaration of AI usage
The dummy data in `./db/dummy_data.sql` is generated by ChatGPT by supplying the schema `./db/schema.sql` and asking ChatGPT to create test data for the given schema. The data given by ChatGPT was slightly modified by me, most notably fixing the foreing key ID's.